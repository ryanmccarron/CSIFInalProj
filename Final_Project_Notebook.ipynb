{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 25096), started 6 days, 22:44:23 ago. (Use '!kill 25096' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8e27378f21d6f9bb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8e27378f21d6f9bb\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "import shutil\n",
    "import os\n",
    "from os import listdir\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from PIL import UnidentifiedImageError\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for reproducibility purposes\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# load tensorboard extension\n",
    "%reload_ext tensorboard\n",
    "# specify the log directory where the tensorboard logs will be written\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7462 images belonging to 16 classes.\n",
      "Found 1859 images belonging to 16 classes.\n",
      "Found 773 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "#commonly used batch size\n",
    "batch_size = 50\n",
    "\n",
    "# Used to augment images, the below augmentations are used to augment the images and output as numeric data for machine learning\n",
    "train_img_gen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    brightness_range=None,\n",
    "    shear_range=0.0,\n",
    "    zoom_range=0.1,\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rescale=1./255,\n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    # splitting 20% of the training data into validation data\n",
    "    validation_split=0.2,\n",
    "    dtype=None\n",
    ")\n",
    "# gets all of the images and labels from the training folder and applies the above augmentations for the training subset of data\n",
    "flowfromframeTrain = train_img_gen.flow_from_directory(\n",
    "    directory='./input/train',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    classes=None,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    save_to_dir=None,\n",
    "    save_prefix='',\n",
    "    save_format='',\n",
    "    follow_links=False,\n",
    "    subset='training',\n",
    "    interpolation='nearest'\n",
    ")\n",
    "    \n",
    "flowfromframeVal = train_img_gen.flow_from_directory(\n",
    "    directory='./input/train',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    classes=None,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    "    save_to_dir=None,\n",
    "    save_prefix='',\n",
    "    save_format='',\n",
    "    follow_links=False,\n",
    "    subset='validation',\n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "test_img_gen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.0,\n",
    "    height_shift_range=0.0,\n",
    "    brightness_range=None,\n",
    "    shear_range=0.0,\n",
    "    zoom_range=0.0,\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=1./255,\n",
    "    preprocessing_function=None,\n",
    "    data_format=None,\n",
    "    validation_split=0.0,\n",
    "    dtype=None\n",
    ")\n",
    "\n",
    "flowfromframeTest = test_img_gen.flow_from_directory(\n",
    "    directory='./input/test',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    classes=None,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    seed=SEED,\n",
    "    save_to_dir=None,\n",
    "    save_prefix='',\n",
    "    save_format='',\n",
    "    follow_links=False,\n",
    "    interpolation='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 85s 844ms/step - loss: 2.6308 - accuracy: 0.2174 - val_loss: 2.3123 - val_accuracy: 0.2432\n",
      "INFO:tensorflow:Assets written to: ./data\\model\\assets\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 84s 837ms/step - loss: 2.1237 - accuracy: 0.3302 - val_loss: 1.9066 - val_accuracy: 0.3920\n",
      "INFO:tensorflow:Assets written to: ./data\\model\\assets\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 86s 850ms/step - loss: 1.9043 - accuracy: 0.4052 - val_loss: 1.6291 - val_accuracy: 0.5112\n",
      "INFO:tensorflow:Assets written to: ./data\\model\\assets\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 85s 840ms/step - loss: 1.7452 - accuracy: 0.4616 - val_loss: 1.6246 - val_accuracy: 0.4952\n",
      "INFO:tensorflow:Assets written to: ./data\\model\\assets\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 85s 843ms/step - loss: 1.6443 - accuracy: 0.4922 - val_loss: 1.4555 - val_accuracy: 0.5280\n",
      "INFO:tensorflow:Assets written to: ./data\\model\\assets\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 84s 835ms/step - loss: 1.5601 - accuracy: 0.5146 - val_loss: 1.2949 - val_accuracy: 0.5936\n",
      "INFO:tensorflow:Assets written to: ./data\\model\\assets\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 85s 842ms/step - loss: 1.4706 - accuracy: 0.5332 - val_loss: 1.2460 - val_accuracy: 0.6000\n",
      "INFO:tensorflow:Assets written to: ./data\\model\\assets\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 85s 844ms/step - loss: 1.3913 - accuracy: 0.5606 - val_loss: 1.3021 - val_accuracy: 0.5904\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 88s 874ms/step - loss: 1.3281 - accuracy: 0.5872 - val_loss: 1.2939 - val_accuracy: 0.5936\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 84s 842ms/step - loss: 1.1071 - accuracy: 0.6594 - val_loss: 1.1445 - val_accuracy: 0.6360\n",
      "INFO:tensorflow:Assets written to: ./data\\model\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x256474aefd0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VGG16 model is initialized here\n",
    "# model = tf.keras.applications.vgg16.VGG16(\n",
    "#     include_top=True,\n",
    "#     weights=None,\n",
    "#     input_tensor=None,\n",
    "#     input_shape=None,\n",
    "#     pooling=None,\n",
    "#     classes=22,\n",
    "#     classifier_activation='softmax'\n",
    "# )\n",
    "input_shape=(224, 224, 3)\n",
    "model = tf.keras.Sequential([\n",
    "  \n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu',input_shape=input_shape),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(16, activation='softmax')\n",
    "])\n",
    "\n",
    "learn_rate = (0.001)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learn_rate)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_Callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3),\n",
    "]\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./data/model',\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\",\n",
    "    options=None,\n",
    "    initial_value_threshold=None,\n",
    ")\n",
    "model_Callbacks.append(model_checkpoint)\n",
    "\n",
    "reduceLRCallback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0.0001,\n",
    ")\n",
    "model_Callbacks.append(reduceLRCallback)\n",
    "\n",
    "numStepsPerEpoch = flowfromframeTrain.total_batches_seen\n",
    "model.fit(\n",
    "    x=flowfromframeTrain,\n",
    "    y=None,\n",
    "    batch_size=batch_size,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    callbacks=model_Callbacks,\n",
    "    validation_split=0.0,\n",
    "    validation_data=flowfromframeVal,\n",
    "    shuffle=False,\n",
    "    class_weight=None,\n",
    "    sample_weight=None,\n",
    "    initial_epoch=0,\n",
    "    steps_per_epoch=100,\n",
    "    validation_steps=25,\n",
    "    validation_freq=1,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 4s 207ms/step - loss: 1.5714 - accuracy: 0.5175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.42      0.50        38\n",
      "           1       0.36      0.50      0.42        38\n",
      "           2       0.61      0.62      0.62        45\n",
      "           3       0.63      0.51      0.56        43\n",
      "           4       0.65      0.74      0.69        57\n",
      "           5       0.49      0.74      0.59        46\n",
      "           6       0.41      0.23      0.30        52\n",
      "           7       0.48      0.51      0.49        41\n",
      "           8       0.62      0.73      0.67        55\n",
      "           9       0.38      0.28      0.32        40\n",
      "          10       0.88      0.83      0.86        54\n",
      "          11       0.46      0.48      0.47        56\n",
      "          12       0.61      0.48      0.54        62\n",
      "          13       0.38      0.44      0.41        54\n",
      "          14       0.24      0.19      0.21        47\n",
      "          15       0.38      0.44      0.41        45\n",
      "\n",
      "    accuracy                           0.52       773\n",
      "   macro avg       0.51      0.51      0.50       773\n",
      "weighted avg       0.52      0.52      0.51       773\n",
      "\n",
      "[[16  0  1  1  4  1  1  1  0  4  0  1  3  2  2  1]\n",
      " [ 0 19  0  1  0  3  1  0  0  1  0  0  1  6  1  5]\n",
      " [ 0  6 28  0  0  0  2  1  0  1  0  1  0  4  0  2]\n",
      " [ 1  1  2 22  1  3  1  2  1  0  1  1  0  1  5  1]\n",
      " [ 2  0  0  2 42  1  0  5  0  1  0  1  1  1  0  1]\n",
      " [ 1  1  0  1  0 34  1  0  6  0  0  2  0  0  0  0]\n",
      " [ 1  2  3  2  0  4 12  0  2  1  0  5  1  5  3 11]\n",
      " [ 0  0  0  0 10  2  2 21  0  1  0  1  3  0  1  0]\n",
      " [ 1  3  0  3  0  3  0  0 40  0  0  1  0  1  3  0]\n",
      " [ 1  1  2  0  2  5  0  1  5 11  2  3  1  1  3  2]\n",
      " [ 0  0  0  0  0  0  2  0  0  1 45  2  1  1  2  0]\n",
      " [ 1  2  3  0  0  7  1  0  1  1  0 27  3  5  3  2]\n",
      " [ 2  0  1  0  6  3  1  6  0  1  2  5 30  1  2  2]\n",
      " [ 0  4  2  2  0  1  1  5  2  2  1  4  0 24  2  4]\n",
      " [ 0  7  1  0  0  3  3  1  6  2  0  5  5  4  9  1]\n",
      " [ 0  7  3  1  0  0  1  1  1  2  0  0  0  8  1 20]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(flowfromframeTest)\n",
    "\n",
    "\n",
    "predictions = model.predict(flowfromframeTest)\n",
    "\n",
    "test_labels = flowfromframeTest.labels\n",
    "\n",
    "y_pred = np.argmax(predictions,axis=-1)\n",
    "\n",
    "print(classification_report(test_labels, y_pred))\n",
    "\n",
    "c_matrix = confusion_matrix(test_labels, y_pred)\n",
    "print(c_matrix)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
